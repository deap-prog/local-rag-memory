# üß† Mon IA Locale avec M√©moire Autonome

Ce projet d√©ploie une stack compl√®te d'IA g√©n√©rative locale pour √©tudiants et d√©veloppeurs. Il combine un syst√®me de chat avanc√© (RAG), une passerelle de s√©curisation et un **archiviste autonome** qui transforme vos conversations en m√©moire √† long terme.

> **Note pour les √©tudiants :** Ce projet est optimis√© pour fonctionner avec un abonnement **Google Gemini Pro** (via l'API) pour une fen√™tre de contexte large, coupl√© √† une recherche web via **Tavily** pour combler les lacunes de connaissances post-2023.

## üèóÔ∏è Architecture

- **AnythingLLM** : Interface de chat et gestionnaire de documents (RAG).
- **LiteLLM** : Passerelle unifi√©e qui centralise les mod√®les (Gemini, Groq, Ollama) et **nettoie les donn√©es sensibles** (anonymisation) avant envoi.
- **Memory Worker** : Script autonome qui :
  1. Archive les conversations √† intervalle r√©gulier.
  2. R√©sume les √©changes via un mod√®le local (Ollama).
  3. Upload les r√©sum√©s dans AnythingLLM (pour vectorisation future).
- **Ollama** : Moteur local pour l'IA de r√©sum√© (gratuit et hors-ligne).

## üöÄ Installation

### Pr√©requis
* [Docker Desktop](https://www.docker.com/products/docker-desktop/) install√©.
* Au moins **16 Go de RAM** (recommand√© pour faire tourner les r√©sum√©s en local).
* Cl√©s API optionnelles (si vous utilisez Groq, Gemini ou OpenAI), sinon tout peut tourner en local via Ollama.

## üöÄ Installation (Windows)

### 1. Pr√©requis
- [Docker Desktop](https://www.docker.com/products/docker-desktop/) install√© et lanc√©.
- Une cl√© API **Google Gemini** (Requis pour le chat principal).
- Une cl√© API **Tavily** (Requis pour la recherche web √† jour).

### 2. D√©marrage Automatique
Nous avons inclus un script pour faciliter l'installation.

1. Double-cliquez sur le fichier `setup.bat`.
   * *Il cr√©era automatiquement le fichier `.env` s'il n'existe pas.*
2. **STOP !** Avant que tout ne d√©marre vraiment, ouvrez le fichier `.env` cr√©√© et **collez vos cl√©s API** (Gemini, Tavily, etc.).
3. Relancez `setup.bat` ou ex√©cutez `docker-compose up -d --build`.

### 3. T√©l√©chargement du mod√®le de r√©sum√©
Le "Memory Worker" utilise une petite IA locale pour r√©sumer vos textes sans frais. Une fois les conteneurs lanc√©s, ouvrez un terminal et tapez :

```bash
docker exec -it ia-ollama ollama pull qwen2.5:3b

### D√©marrage Rapide

1.  **Cloner le projet :**
    ```bash
    git clone [https://github.com/votre-pseudo/votre-repo.git](https://github.com/votre-pseudo/votre-repo.git)
    cd votre-repo
    ```

2.  **Initialisation :**
    * **Windows** : Double-cliquez sur `setup.bat` (si disponible) ou copiez `.env.example` en `.env`.
    * **Linux/Mac** :
        ```bash
        cp .env.example .env
        # Ou lancez ./setup.sh si fourni
        ```

3.  **Configuration :**
    Ouvrez le fichier `.env` g√©n√©r√© et renseignez vos cl√©s API (Groq, Gemini...) ou laissez vide si vous n'utilisez que du local. D'autres param√®tres sont aussi configurable.

4.  **Lancement :**
    ```bash
    docker-compose up -d --build
    ```

5.  **üì• T√©l√©chargement des mod√®les (Important) :**
    Une fois les conteneurs lanc√©s, vous devez t√©l√©charger les mod√®les pour Ollama (utilis√©s par le Memory Worker).

    Ex√©cutez cette commande dans votre terminal :
    ```bash
    docker exec -it ia-ollama ollama pull qwen2.5:3b
    docker exec -it ia-ollama ollama pull mistral
    ```
    *Note : Le mod√®le `qwen2.5:3b` est configur√© par d√©faut pour les r√©sum√©s dans le `.env`.*

## üîó Acc√®s aux Interfaces

| Service | URL | Description |
| :--- | :--- | :--- |
| **AnythingLLM** | http://localhost:23001 | Chat principal avec m√©moire vectorielle |
| **Open WebUI** | http://localhost:23002 | Chat secondaire (UI alternative gpt like) |
| **LiteLLM Proxy** | http://localhost:23003 | API compatible OpenAI (Port 4000 interne) |
| **Ollama API** | http://localhost:23004 | API du moteur local |

## üìÇ Fonctionnement de la M√©moire

Vos donn√©es sont stock√©es localement :
* Base de donn√©es : `./data/memory/anythingllm.db`
* Archives brutes : `./data/memory-worker/archives/`
* R√©sum√©s Markdown : `./data/memory-worker/markdowns/`

**Cycle de vie du Memory Worker :**
1.  √Ä intervalle r√©gulier (configur√© dans `.env`, ex: 04h00 du matin), le worker se r√©veille.
2.  Il d√©tecte les nouvelles conversations.
3.  Il g√©n√®re un r√©sum√© concis via le mod√®le local (Qwen/Phi).
4.  Il upload ce r√©sum√© dans AnythingLLM.
5.  **R√©sultat :** Le lendemain, vous pouvez demander √† l'IA : *"De quoi avons-nous parl√© hier concernant le projet X ?"* et elle saura vous r√©pondre.

## üõ†Ô∏è Personnalisation

Le fichier `.env` contr√¥le la majorit√© des param√®tres :
* `SUMMARY_TIME` : Heure du r√©sum√© automatique.
* `BASE_MODEL` : Mod√®le utilis√© pour r√©sumer (doit √™tre l√©ger, ex: qwen2.5:3b).
* `WORD_LIMIT` : Longueur max des r√©sum√©s.

‚öôÔ∏è Configuration AnythingLLM (Tuto)

Une fois l'installation termin√©e, acc√©dez √† http://localhost:23001. Vous devez configurer le logiciel pour qu'il utilise notre architecture.
√âtape 1 : Connecter l'IA (LiteLLM)

Au lieu de connecter Gemini directement, nous passons par notre proxy s√©curis√© LiteLLM.

    Allez dans les Settings (roue dent√©e) > LLM Preference.

    Dans la liste des fournisseurs, choisissez LiteLLM (ou "Generic OpenAI").

    Base URL : Entrez exactement cette adresse (c'est l'adresse interne du r√©seau Docker) : http://ia-litellm:4000/v1

    API Key : Mettez n'importe quoi (ex: sk-fake), LiteLLM g√®re les vraies cl√©s.

    Chat Model ID : S√©lectionnez le mod√®le souhait√© (ex: gemini/gemini-1.5-pro ou groq/llama-3).

√âtape 2 : Activer la Recherche Web (Indispensable)

Les mod√®les IA ont une connaissance arr√™t√©e dans le pass√© (2023/2024). Pour qu'ils puissent r√©pondre sur l'actualit√© ou des docs r√©cents :

    Allez dans Agent Skills (ou "Tools").

    Activez Web Search.

    Choisissez Tavily comme moteur de recherche.

    Entrez votre cl√© API Tavily.

√âtape 3 : La M√©moire (Vectorisation Manuelle)

Le "Memory Worker" upload automatiquement les r√©sum√©s de vos conversations pr√©c√©dentes dans votre Workspace, mais il ne lance pas le calcul vectoriel lourd.

    De temps en temps, allez dans les param√®tres de votre Workspace.

    V√©rifiez la liste des documents : vous verrez des fichiers .md ajout√©s par le worker (ex: conversation_summary.md).

    Cliquez sur "Save and Embed" (ou "Re-embed") pour que l'IA "apprenne" ces nouveaux souvenirs.

        Pourquoi manuel ? Cela √©vite de surcharger votre processeur √† chaque petit r√©sum√© et vous permet de v√©rifier ce qui est ajout√© √† la m√©moire.

üîó Acc√®s Rapides

    Interface Chat : http://localhost:23001

    LiteLLM API : http://localhost:23003

    Dossier des donn√©es : Les conversations sont stock√©es localement dans ./data/.
